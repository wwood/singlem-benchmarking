
import extern
import os

# Add the parent directory to sys.path
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(os.path.abspath(workflow.snakefile)), '..')))
from tool_reference_data import *

num_threads = config['benchmarking_threads']
kraken_num_threads = 999 # never let kraken be parallelised, because otherwise it will exhaust the RAM.
if 'kraken_threads' in config:
    kraken_num_threads = config['kraken_threads']

tools = ['singlem', 'metaphlan', 'motus', 'kraken', 'sourmash', 'map2b', 'kaiju', 'metabuli']

output_prefix = 'output_'
output_dirs = list([output_prefix+tool for tool in tools])
output_dirs_dict = dict(zip(tools, output_dirs))

benchmark_dir = 'benchmarks'

datasets = list(['marine'+str(i) for i in range(10)])
datasets = datasets[:5] # Just use 5, 10 is unnecessary

known_species_fractions = [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]

fastq_dir = 'local_reads'
truth_dir = os.path.join(workflow.basedir, 'truths')

known_genome_fasta_paths = os.path.join(workflow.basedir, 'shadow_genome_paths.csv')
coverage_definitions_folder = 'coverage_definitions'

novel_genomes_gtdbtk_output_directory = 'gtdbtk_batchfile.random1000.gtdbtk_r207'
novel_genome_list = 'gtdbtk_batchfile.random1000.csv'

# Only GTDB R207 tools
tools = r207_tools

# debug
# datasets = datasets = datasets[:1]
# tools = ['singlem']
# known_species_fractions = [10]
# tools = ['singlem', 'metaphlan', 'sourmash', 'metabuli']

##################################################################### things that should be in config

gtdb_bac_taxonomy = '../bac120_taxonomy_r207.tsv'
gtdb_ar_taxonomy = '../ar53_taxonomy_r207.tsv'

##################################################################### reference databases

# metapackage = '/work/microbiome/msingle/mess/163_S3.1.1_metapackage/output_dir/metapackage/S3.1.1.GTDB_r207.metapackage_20240302.smpkg'
singlem_metapackage_local = join(output_dirs_dict['singlem'], 'data', os.path.basename(singlem_metapackage))

# metaphlan_db_original1 = '/work/microbiome/msingle/mess/115_camisim_ish_benchmarking/metaphlan_bowtiedb'
metaphlan_db_local1 = output_dirs_dict['metaphlan'] + '/metaphlan/data/metaphlan_bowtiedb'

# motus_db_path_original = '/work/microbiome/msingle/mess/124_singlem-benchmarking/db_mOTU'
# motus_db_path_local = output_dirs_dict['motus'] + '/motus/data/db_mOTU'
# motus_gtdb_tsv = '/work/microbiome/msingle/mess/115_camisim_ish_benchmarking/motus/mOTUs_3.0.0_GTDB_tax.tsv'

# kraken_db = "/work/microbiome/db/struo2/GTDB_release207"
kraken_db_local = output_dirs_dict['kraken'] + "/kraken/data/GTDB_release207"

# sourmash_db1_original = '/work/microbiome/msingle/mess/105_novelty_testing/gtdb-rs207.taxonomy.sqldb'
# sourmash_db2_original = '/work/microbiome/msingle/mess/105_novelty_testing/gtdb-rs207.genomic-reps.dna.k31.zip'
sourmash_db_taxonomy_local = output_dirs_dict['sourmash'] + '/sourmash/data/gtdb-rs207.taxonomy.sqldb'
sourmash_db_dna_local = output_dirs_dict['sourmash'] + '/sourmash/data/gtdb-rs207.genomic-reps.dna.k31.zip'

# kaiju_db = output_dirs_dict['kaiju'] + '/kaiju/data/kaiju_db_progenomes.fmi'
# kaiju_db_progenomes_nodes = output_dirs_dict['kaiju'] + '/kaiju/data/nodes.dmp'
# kaiju_db_progenomes_names = output_dirs_dict['kaiju'] + '/kaiju/data/names.dmp'

# map2b_checkout_dir = '/home/woodcrob/bioinfo/MAP2B'
# map2b_db = os.path.join(map2b_checkout_dir, 'database.bak/GTDB')
# map2b_db_local = output_dirs_dict['map2b'] + '/map2b/data/GTDB'

# metabuli_db = '/work/microbiome/db/metabuli/gtdb207'
metabuli_db_local = output_dirs_dict['metabuli'] + '/metabuli/data/gtdb207'

#####################################################################


rule all:
    input:
        expand(output_prefix+"{tool}/opal/known{known_percent}/{sample}.opal_report", sample=datasets, tool=tools, known_percent=known_species_fractions),

rule all_reads:
	input:
		expand(fastq_dir + "/known{known_percent}/{sample}.finished", sample=datasets, known_percent=known_species_fractions)
	output:
		touch(fastq_dir+'/all.done')

rule generate_community_and_reads:
    output:
        r1=fastq_dir + "/known{known_percent}/{sample}.1.fq.gz",
        r2=fastq_dir + "/known{known_percent}/{sample}.2.fq.gz",
        condensed = os.path.join(truth_dir, "known{known_percent}/{sample}.condensed"),
        done = touch(fastq_dir + "/known{known_percent}/{sample}.finished")
    params:
        coverage_number = lambda wildcards: wildcards.sample.replace('marine', ''),
    log:
        fastq_dir + "/known{known_percent}/{sample}.log"
    threads: 8
    resources:
        mem_mb=8000,
        runtime='24h'
    conda:
        'envs/art.yml'
    shell:
        "{workflow.basedir}/generate_community.py " \
        " --art art_illumina " \
        " --threads {threads}" \
        " --coverage-file {workflow.basedir}/coverage_definitions/coverage{params.coverage_number}.tsv" \
        " --gtdb-bac-metadata {gtdb_bac_metadata} --gtdb-ar-metadata {gtdb_ar_metadata}" \
        " --known-genome-list {known_genome_fasta_paths}" \
        " --novel-genome-gtdbtk-output {novel_genomes_gtdbtk_output_directory}" \
        " --novel-genome-list {novel_genome_list}" \
        " --output-condensed {output.condensed}" \
        " -1 {fastq_dir}/known{wildcards.known_percent}/{wildcards.sample}.1.fq.gz"  \
        " -2 {fastq_dir}/known{wildcards.known_percent}/{wildcards.sample}.2.fq.gz" \
        " --percent-known {wildcards.known_percent}" \
        " 2> {log}"

rule truth_condensed_to_biobox:
    input:
        condensed = os.path.join(truth_dir, "known{known_percent}/{sample}.condensed"),
    output:
        biobox = os.path.join(truth_dir, "known{known_percent}/{sample}.condensed.biobox"),
    conda:
        #"envs/singlem.yml"
        'singlem-dev' # Need changes post 0.16
    resources:
        mem_mb=4000,
        runtime='1h'
    shell:
        "PYTHONPATH=../singlem {workflow.basedir}/../bin/condensed_profile_to_biobox.py --input-condensed-table {input.condensed} " \
        "--output-biobox {output.biobox}"

def get_condensed_to_biobox_extra_args(tool):
    if tool in tools_with_filled_output_profiles:
        return ' --no-fill'
    else:
        return ''

rule tool_condensed_to_biobox:
    input:
        profile = output_prefix + "{tool}/{tool}/known{known_percent}/{sample}.profile",
        truth = os.path.join(truth_dir, "known{known_percent}/{sample}.condensed.biobox"),
    params:
        extra_args = lambda wildcards: get_condensed_to_biobox_extra_args(wildcards.tool)
    output:
        biobox = output_prefix + "{tool}/biobox/known{known_percent}/{sample}.biobox",
    conda:
        "envs/singlem.yml"
    resources:
        mem_mb=4000,
        runtime='1h'
    shell:
        "{workflow.basedir}/../bin/condensed_profile_to_biobox.py {params.extra_args} --input-condensed-table {input.profile} " \
        "--output-biobox {output.biobox} --template-biobox {input.truth} "

rule opal:
    input:
        biobox = output_prefix + "{tool}/biobox/known{known_percent}/{sample}.biobox",
        truth = os.path.join(truth_dir, "known{known_percent}/{sample}.condensed.biobox"),
    params:
        output_opal_dir = output_prefix+"{tool}/opal/known{known_percent}/{sample}.opal_output_directory",
    output:
        report=output_prefix+"{tool}/opal/known{known_percent}/{sample}.opal_report",
        done=output_prefix+"{tool}/opal/known{known_percent}/{sample}.opal_report.done",
    conda:
        'envs/opal.yml'
    resources:
        mem_mb=4000,
        runtime='1h'
    shell:
        "opal.py -g {input.truth} -o {params.output_opal_dir} {input.biobox} || echo 'expected opal non-zero existatus'; mv {params.output_opal_dir}/results.tsv {output.report} && rm -rf {params.output_opal_dir} && touch {output.done}"


###############################################################################################
###############################################################################################
###############################################################################################
#########
######### tool-specific rules - singlem first

rule singlem_copy_metapackage:
    input:
        singlem_metapackage
    output:
        db=directory(singlem_metapackage_local),
        done=output_dirs_dict['singlem'] + "/singlem/data/done",
    resources:
        mem_mb=4000,
        runtime='1h'
    shell:
        "cp -rL {input} {output.db} && touch {output.done}"

rule singlem_run_to_profile:
    input:
        r1=fastq_dir + "/known{known_percent}/{sample}.1.fq.gz",
        r2=fastq_dir + "/known{known_percent}/{sample}.2.fq.gz",
        db=singlem_metapackage,
        data_done=output_dirs_dict['singlem'] + "/singlem/data/done"
    benchmark:
        benchmark_dir + "/singlem/known{known_percent}/{sample}-"+str(num_threads)+"threads.benchmark"
    output:
        report=output_dirs_dict['singlem'] + "/singlem/known{known_percent}/{sample}.profile",
        done=touch(output_dirs_dict['singlem'] + "/singlem/known{known_percent}/{sample}.profile.done")
    conda:
        "envs/singlem.yml"
    threads:
        num_threads
    resources:
        mem_mb=8000,
        runtime='2h'
    log:
        output_dirs_dict['singlem'] + "/logs/singlem/known{known_percent}/{sample}.log"
    shell:
        "singlem pipe --threads {threads} -1 {input.r1} -2 {input.r2} -p {output.report} --metapackage {input.db} &> {log}"


###############################################################################################
###############################################################################################
###############################################################################################
#########
######### metaphlan


rule metaphlan_copy_db:
    # input:
    # Cannot use the directory as input/output because humann complains when
    # there's a snakemake hidden file in the dir
    # db1=directory(metaphlan_db_original1),
    # db2=directory(metaphlan_db_original2),
    output:
        # db1=directory(metaphlan_db_local1),
        # db2=directory(metaphlan_db_local2),
        done=touch(output_dirs_dict['metaphlan'] + "/metaphlan/data/done")
    resources:
        mem_mb=4000,
        runtime='1h'
    shell:
        "cp -rvL {metaphlan_db} {metaphlan_db_local1}"

rule metaphlan_profile:
    input:
        r1=fastq_dir + "/known{known_percent}/{sample}.1.fq.gz",
        r2=fastq_dir + "/known{known_percent}/{sample}.2.fq.gz",
        done=output_dirs_dict['metaphlan'] + "/metaphlan/data/done"
    benchmark:
        benchmark_dir + "/metaphlan/known{known_percent}/{sample}-"+str(num_threads)+"threads.benchmark"
    output:
        sgb_report=output_dirs_dict['metaphlan'] + "/metaphlan/known{known_percent}/{sample}.sgb_report",
        done=touch(output_dirs_dict['metaphlan'] + "/metaphlan/known{known_percent}/{sample}.profile.done")
    conda:
        "envs/metaphlan.yml"
    threads: num_threads
    resources:
        mem_mb=32000,
        runtime='6h'
    params:
        cat_reads = output_dirs_dict['metaphlan'] + "/metaphlan/known{known_percent}/{sample}.cat.fq.gz",
    log:
        output_dirs_dict['metaphlan'] + "/logs/metaphlan/known{known_percent}/{sample}.log"
    shell:
        # Concatenate input files because metaphlan can't handle multiple input files
        "rm -f {output.sgb_report} {params.cat_reads}.bowtie2out.txt; cat {input.r1} {input.r2} > {params.cat_reads} && metaphlan {params.cat_reads} --index {metaphlan_index} --nproc {threads} --input_type fastq --bowtie2db {metaphlan_db_local1} -o {output.sgb_report} &> {log}"

rule metaphlan_convert_profile_to_GTDB:
    input:
        report=output_dirs_dict['metaphlan'] + "/metaphlan/known{known_percent}/{sample}.sgb_report"
    output:
        gtdb_report=output_dirs_dict['metaphlan'] + "/metaphlan/known{known_percent}/{sample}.gtdb_profile",
        done=touch(output_dirs_dict['metaphlan'] + "/metaphlan/known{known_percent}/{sample}.gtdb_report.done")
    conda:
        "envs/metaphlan.yml"
    resources:
        mem_mb=4000,
        runtime='1h'
    log:
        output_dirs_dict['metaphlan'] + "/logs/metaphlan/known{known_percent}/{sample}-convert.log"
    shell:
        "sgb_to_gtdb_profile.py -i {input.report} -o {output.gtdb_report} -d {metaphlan_db_local1}/mpa_vOct22_CHOCOPhlAnSGB_202212.pkl &> {log}"

rule metaphlan_profile_to_condensed:
    input:
        report=output_dirs_dict['metaphlan'] + "/metaphlan/known{known_percent}/{sample}.gtdb_profile"
    output:
        profile = output_dirs_dict['metaphlan'] + "/metaphlan/known{known_percent}/{sample}.profile",
    conda:
        "envs/singlem.yml"
    resources:
        mem_mb=4000,
        runtime='1h'
    shell:
        "{workflow.basedir}/../bin/metaphlan_to_condensed.py --metaphlan {input} --sample {wildcards.sample} > {output.profile} "

###############################################################################################
###############################################################################################
###############################################################################################
#########
######### motus

# rule motus_copy_db:
#     input:
#         db=motus_db_path_original,
#     output:
#         db=directory(motus_db_path_local),
#         done=touch(output_dirs_dict['motus'] + "/motus/data/done")
#     resources:
#         mem_mb=4000,
#         runtime='1h'
#     shell:
#         "mkdir -p {output.db} && rmdir {output.db} && cp -rvL {input.db} {output.db}"

# rule motus_run:
#     input:
#         r1=fastq_dir + "/known{known_percent}/{sample}.1.fq.gz",
#         r2=fastq_dir + "/known{known_percent}/{sample}.2.fq.gz",
#         done=output_dirs_dict['motus'] + "/motus/data/done"
#     benchmark:
#         benchmark_dir + "/motus/known{known_percent}/{sample}-"+str(num_threads)+"threads.benchmark"
#     output:
#         report=output_dirs_dict['motus'] + "/motus/known{known_percent}/{sample}.motus",
#         done=touch(output_dirs_dict['motus'] + "/motus/known{known_percent}/{sample}.profile.done")
#     threads: num_threads
#     resources:
#         mem_mb=30000,
#         runtime='6h'
#     conda:
#         "envs/motus.yml"
#     log:
#         output_dirs_dict['motus'] + "/logs/motus/known{known_percent}/{sample}.log"
#     shell:
#         "motus profile -t {threads} -db {motus_db_path_local} -f {input.r1} -r {input.r2} -o {output.report} &> {log}"

# rule motus_profile_to_condensed:
#     input:
#         report=output_dirs_dict['motus'] + "/motus/known{known_percent}/{sample}.motus"
#     output:
#         profile = output_dirs_dict['motus'] + "/motus/known{known_percent}/{sample}.profile",
#     conda:
#         "envs/singlem.yml"
#     resources:
#         mem_mb=4000,
#         runtime='1h'
#     shell:
#         "{workflow.basedir}/../bin/motus_to_condensed.py --motus {input.report} " \
#         "--gtdb {motus_gtdb_tsv} " \
#         " > {output.profile} "


###############################################################################################
###############################################################################################
###############################################################################################
######### bracken


rule braken_copy_data:
    output:
        db=directory(kraken_db_local),
        done=output_dirs_dict['kraken'] + "/kraken/data/done"
    resources:
        mem_mb=4000,
        runtime='1h'
    shell:
        "cp -r {kraken_db} {output.db}/ && touch {output.done}"

rule kraken_run:
    input:
        reads_copied1 = fastq_dir + "/known{known_percent}/{sample}.1.fq.gz",
        reads_copied2 = fastq_dir + "/known{known_percent}/{sample}.2.fq.gz",
        db=kraken_db_local,
        copy_braken_data_done=output_dirs_dict['kraken'] + "/kraken/data/done"
    benchmark:
        benchmark_dir + "/kraken/known{known_percent}/{sample}-"+str(num_threads)+"threads.benchmark"
    threads: min(kraken_num_threads, num_threads)
    resources:
        mem_mb=400000,
        runtime='1h'
    output:
        report=output_dirs_dict['kraken'] + "/kraken/known{known_percent}/{sample}.kraken",
        done=touch(output_dirs_dict['kraken'] + "/kraken/known{known_percent}/{sample}.kraken.done")
    conda:
        "envs/kraken.yml"
    log:
        output_dirs_dict['kraken'] + "/logs/kraken/known{known_percent}/{sample}.log"
    shell:
        "kraken2 --db {input.db} --threads {num_threads} --output /dev/null --report {output.report} --paired {input.reads_copied1} {input.reads_copied2} &> {log}"

rule braken_run:
    input:
        db=kraken_db_local,
        kraken_report=output_dirs_dict['kraken'] + "/kraken/known{known_percent}/{sample}.kraken",
        kraken_done=output_dirs_dict['kraken'] + "/kraken/known{known_percent}/{sample}.kraken.done"
    output:
        s=output_dirs_dict['kraken'] + "/braken/known{known_percent}/{sample}.report.S",
        g=output_dirs_dict['kraken'] + "/braken/known{known_percent}/{sample}.report.G",
        f=output_dirs_dict['kraken'] + "/braken/known{known_percent}/{sample}.report.F",
        o=output_dirs_dict['kraken'] + "/braken/known{known_percent}/{sample}.report.O",
        c=output_dirs_dict['kraken'] + "/braken/known{known_percent}/{sample}.report.C",
        p=output_dirs_dict['kraken'] + "/braken/known{known_percent}/{sample}.report.P",
        d=output_dirs_dict['kraken'] + "/braken/known{known_percent}/{sample}.report.D",
        done=touch(output_dirs_dict['kraken'] + "/braken/known{known_percent}/{sample}.done")
    conda:
        "envs/bracken.yml"
    resources:
        mem_mb=4000,
        runtime='1h'
    log:
        output_dirs_dict['kraken'] + "/logs/bracken/known{known_percent}/{sample}.log"
    shell:
        "bash -c 'bracken -d {input.db} -r 150 -l S -t 10 -o {output.s} -i {input.kraken_report} && " \
        "bracken -d {input.db} -r 150 -l G -t 10 -o {output.g} -i {input.kraken_report} && " \
        "bracken -d {input.db} -r 150 -l F -t 10 -o {output.f} -i {input.kraken_report} && " \
        "bracken -d {input.db} -r 150 -l O -t 10 -o {output.o} -i {input.kraken_report} && " \
        "bracken -d {input.db} -r 150 -l C -t 10 -o {output.c} -i {input.kraken_report} && " \
        "bracken -d {input.db} -r 150 -l P -t 10 -o {output.p} -i {input.kraken_report} && " \
        "bracken -d {input.db} -r 150 -l D -t 10 -o {output.d} -i {input.kraken_report}' &> {log}"


rule bracken_to_profile:
    input:
        output_dirs_dict['kraken'] + "/braken/known{known_percent}/{sample}.report.S",
        output_dirs_dict['kraken'] + "/braken/known{known_percent}/{sample}.report.G",
        output_dirs_dict['kraken'] + "/braken/known{known_percent}/{sample}.report.F",
        output_dirs_dict['kraken'] + "/braken/known{known_percent}/{sample}.report.O",
        output_dirs_dict['kraken'] + "/braken/known{known_percent}/{sample}.report.C",
        output_dirs_dict['kraken'] + "/braken/known{known_percent}/{sample}.report.P",
        output_dirs_dict['kraken'] + "/braken/known{known_percent}/{sample}.report.D",
        output_dirs_dict['kraken'] + "/braken/known{known_percent}/{sample}.done"
    params:
        report_prefix = output_dirs_dict['kraken']+"/braken/known{known_percent}/{sample}.report",
        biobox_dir = output_dirs_dict['kraken']+'/biobox',
        truth = truth_dir + "/known{known_percent}/{sample}.condensed.biobox",
    output:
        profile = output_dirs_dict['kraken'] + "/kraken/known{known_percent}/{sample}.profile",
    conda:
        "singlem-dev"
    resources:
        mem_mb=4000,
        runtime='1h'
    log:
        output_dirs_dict['kraken'] + "/logs/bracken_to_profile/known{known_percent}/{sample}.log"
    shell:
        # Convert reports to singlem condense format
        "mkdir -p {params.biobox_dir} && " \
        "{workflow.basedir}/../bin/kraken_to_condensed.py --report-prefix {params.report_prefix} " \
        "--bacterial-taxonomy ../bac120_taxonomy_r207.tsv " \
        "--archaeal-taxonomy ../ar53_taxonomy_r207.tsv > {output.profile} 2> {log}"

###############################################################################################
###############################################################################################
###############################################################################################
######### sourmash


rule copy_db:
    input:
        db1 = sourmash_db_taxonomy,
        db2 = sourmash_db_dna,
    output:
        db1=sourmash_db_taxonomy_local,
        db2=sourmash_db_dna_local,
        done=output_dirs_dict['sourmash'] + "/sourmash/data/done"
    resources:
        mem_mb=4000,
        runtime='1h'
    params:
        output_dir = output_dirs_dict['sourmash'],
    shell:
        "mkdir -p {params.output_dir}/sourmash/data && cp -rvL {input.db1} {output.db1} && cp -rvL {input.db2} {output.db2} && touch {output.done}"

rule sourmash_run:
    input:
        r1=fastq_dir + "/known{known_percent}/{sample}.1.fq.gz",
        r2=fastq_dir + "/known{known_percent}/{sample}.2.fq.gz",
        db1=sourmash_db_taxonomy_local,
        db2=sourmash_db_dna_local,
        done=output_dirs_dict['sourmash'] + "/sourmash/data/done"
    benchmark:
        benchmark_dir + "/sourmash/known{known_percent}/{sample}-"+str(num_threads)+"threads.benchmark"
    threads: num_threads
    resources:
        mem_mb=60000,
        runtime='2h'
    output:
        report=output_dirs_dict['sourmash'] + "/sourmash/known{known_percent}/{sample}.csv",
        done=touch(output_dirs_dict['sourmash'] + "/sourmash/known{known_percent}/{sample}.profile.done")
    conda:
        "envs/sourmash.yml"
    params:
        output_dir = output_dirs_dict['sourmash'],
        sourmash_prefix = lambda wildcards: output_dirs_dict['sourmash'] + f"/sourmash/known{wildcards.known_percent}/"+wildcards.sample
    log:
        output_dirs_dict['sourmash'] + "/logs/sourmash/known{known_percent}/{sample}.log"
    shell:
        # Sourmash does not seem to have a --threads option
        "bash -c 'sourmash sketch dna -p k=31,abund --merge blah -o {params.sourmash_prefix}.sig {input.r1} {input.r2} "\
        "&& echo \"running gather..\" "\
        "&& sourmash gather -k 31 --dna {params.sourmash_prefix}.sig {sourmash_db_dna} -o {params.sourmash_prefix}.gather_gtdbrs207_reps.csv "\
        "&& echo \"running tax ..\" "\
        "&& sourmash tax metagenome -g {params.sourmash_prefix}.gather_gtdbrs207_reps.csv -t {sourmash_db_taxonomy} >{output.report}' &> {log}"

rule sourmash_to_condensed:
    input:
        output_dirs_dict['sourmash'] + "/sourmash/known{known_percent}/{sample}.csv",
    output:
        profile = output_dirs_dict['sourmash'] + "/sourmash/known{known_percent}/{sample}.profile",
    resources:
        mem_mb=4000,
        runtime='1h'
    shell:
        "{workflow.basedir}/../bin/sourmash_to_condensed.py --summary-csv {input} " \
        "--sample {wildcards.sample} " \
        " > {output} "

###############################################################################################
###############################################################################################
###############################################################################################
######### kaiju

# rule kaiju_copy_db:
#     # input:
#     #     db = kaiju_db_original_progenomes_fmi,
#     #     nodes = kaiju_db_original_progenomes_nodes,
#     #     names = kaiju_db_original_progenomes_names,
#     output:
#         db=kaiju_db,
#         nodes = kaiju_db_progenomes_nodes,
#         names = kaiju_db_progenomes_names,
#         done=output_dirs_dict['kaiju'] + "/kaiju/data/done"
#     resources:
#         mem_mb=4000,
#         runtime='1h'
#     shell:
#         "mkdir -p {output_dirs_dict[kaiju]}/kaiju/data && " \
#         "cp -rvL {kaiju_db_original_progenomes_fmi} {output.db} " \
#         "&& cp -rvL {kaiju_db_original_progenomes_nodes} {output.nodes} " \
#         "&& cp -rvL {kaiju_db_original_progenomes_names} {output.names} "\
#         "&& touch {output.done}"
        
# rule kaiju_run:
#     input:
#         r1=fastq_dir + "/known{known_percent}/{sample}.1.fq.gz",
#         r2=fastq_dir + "/known{known_percent}/{sample}.2.fq.gz",
#         db=kaiju_db,
#         nodes = kaiju_db_progenomes_nodes,
#         names = kaiju_db_progenomes_names,
#         done=output_dirs_dict['kaiju'] + "/kaiju/data/done"
#     benchmark:
#         benchmark_dir + "/kaiju/known{known_percent}/{sample}-"+str(num_threads)+"threads.benchmark"
#     threads: num_threads
#     resources:
#         mem_mb=30000,
#         runtime='1h'
#     output:
#         report=output_dirs_dict['kaiju'] + "/kaiju-run/known{known_percent}/{sample}.report",
#         done=touch(output_dirs_dict['kaiju'] + "/kaiju/known{known_percent}/{sample}.profile.done")
#     conda:
#         "envs/kaiju.yml"
#     log:
#         output_dirs_dict['kaiju'] + "/logs/kaiju/known{known_percent}/{sample}.log"
#     shell:
#         # kaiju -z 25 -t nodes.dmp -f kaiju_db_progenomes.fmi -i <(zcat ~/m/msingle/mess/124_singlem-benchmarking/2_phylogenetic_novelty/local_reads/GCA_020697715.1_genomic.1.fq.gz ~/m/msingle/mess/124_singlem-benchmarking/2_phylogenetic_novelty/local_reads/GCA_020697715.1_genomic.2.fq.gz) -o kaiju.out
#         "kaiju -z {threads} -t {input.nodes} -f {input.db} -i <(zcat {input.r1} {input.r2}) -o {output.report} &> {log}"

# rule kaiju_to_summarised_table:
#     input:
#         output_dirs_dict['kaiju'] + "/kaiju-run/known{known_percent}/{sample}.report",
#     output:
#         profile = output_dirs_dict['kaiju'] + "/kaiju/known{known_percent}/{sample}.summary.report",
#         done=touch(output_dirs_dict['kaiju'] + "/kaiju/known{known_percent}/{sample}.summary.report.done")
#     conda:
#         "envs/kaiju.yml"
#     resources:
#         mem_mb=4000,
#         runtime='1h'
#     log:
#         output_dirs_dict['kaiju'] + "/logs/kaiju_to_summarised_table/known{known_percent}/{sample}.log"
#     shell:
#         # kaiju2table -t nodes.dmp -n names.dmp -r genus -o kaiju_summary.tsv kaiju.out
#         "kaiju2table -t {kaiju_db_progenomes_nodes} -n {kaiju_db_progenomes_names} -r phylum -o {output.profile} {input} &> {log}"

# rule kaiju_to_condensed:
#     input:
#         output_dirs_dict['kaiju'] + "/kaiju/known{known_percent}/{sample}.summary.report",
#     output:
#         profile = output_dirs_dict['kaiju'] + "/kaiju/known{known_percent}/{sample}.profile",
#     conda:
#         "envs/kaiju_to_kingdom.yml"
#     resources:
#         mem_mb=4000,
#         runtime='1h'
#     shell:
#         "{workflow.basedir}/../bin/kaiju_to_kingdom_condensed.py --input {input} " \
#         "--sample {wildcards.sample} " \
#         "--data-dir {kaiju_db_original_dir} " \
#         " > {output} "


###############################################################################################
###############################################################################################
###############################################################################################
######### MAP2B

# rule map2b_copy_data:
#     output:
#         db=directory(map2b_db_local),
#         done=touch(output_dirs_dict['map2b'] + "/map2b/data/done")
#     resources:
#         mem_mb=4000,
#         runtime='1h'
#     shell:
#         "cp -r {map2b_db} {output.db}/"

# rule map2b_sample_setup:
#     input:
#         r1=fastq_dir + "/known{known_percent}/{sample}.1.fq.gz",
#         r2=fastq_dir + "/known{known_percent}/{sample}.2.fq.gz",
#     output:
#         listfile=output_dirs_dict['map2b'] + "/map2b/listfiles/known{known_percent}/{sample}",
#         done=touch(output_dirs_dict['map2b'] + "/map2b/done/setup/known{known_percent}/{sample}.done")
#     resources:
#         mem_mb=4000,
#         runtime='1h'
#     shell:
#         "echo {wildcards.sample} {input.r1} {input.r2} |sed 's/ /\t/g' > {output.listfile}"

# rule map2b_run:
#     input:
#         listfile=output_dirs_dict['map2b'] + "/map2b/listfiles/known{known_percent}/{sample}",
#         done=output_dirs_dict['map2b'] + "/map2b/data/done"
#     output:
#         tsv=output_dirs_dict['map2b'] + "/map2b/known{known_percent}/{sample}/Abundance.xls",
#         done=touch(output_dirs_dict['map2b'] + "/map2b/done/run/known{known_percent}/{sample}.done")
#     threads: num_threads
#     resources:
#         mem_mb=16000,
#         runtime='12h'
#     benchmark:
#         benchmark_dir + "/map2b/known{known_percent}/{sample}-"+str(num_threads)+"threads.benchmark"
#     conda:
#         "envs/MAP2B-20230420-conda.yml"
#     log:
#         output_dirs_dict['map2b'] + "/logs/map2b/known{known_percent}/{sample}.log"
#     params:
#         output_dir = lambda wildcards, output: os.path.dirname(output.tsv),
#     shell:
#         "python3 {map2b_checkout_dir}/bin/MAP2B.py -i {input.listfile} -d {map2b_db_local} -o {params.output_dir} -p {threads} &> {log}"

# rule map2b_to_biobox:
#     input:
#         tsv=output_dirs_dict['map2b'] + "/map2b/known{known_percent}/{sample}/Abundance.xls",
#     output:
#         profile = output_dirs_dict['map2b'] + "/map2b/known{known_percent}/{sample}.profile",
#     resources:
#         mem_mb=4000,
#         runtime='1h'
#     shell:
#         "{workflow.basedir}/../bin/map2b_to_condensed.py "
#         "--abundance {input} "
#         "--sample {wildcards.sample} "
#         "> {output} "

###############################################################################################
###############################################################################################
###############################################################################################
######### metabuli

rule metabuli_copy_db:
    input:
        db=metabuli_db_dir,
    output:
        db=directory(metabuli_db_local),
        done=touch(output_dirs_dict['metabuli'] + "/metabuli/data/done")
    resources:
        mem_mb=4000,
        runtime='1h'
    shell:
        "cp -r {input.db} {output.db}"

rule metabuli_run:
    input:
        r1=fastq_dir + "/known{known_percent}/{sample}.1.fq.gz",
        r2=fastq_dir + "/known{known_percent}/{sample}.2.fq.gz",
        db=metabuli_db_local,
        done=output_dirs_dict['metabuli'] + "/metabuli/data/done"
    output:
        report=output_dirs_dict['metabuli'] + "/output/known{known_percent}/{sample}_report.tsv",
        done=touch(output_dirs_dict['metabuli'] + "/output/known{known_percent}/{sample}.done")
    threads: num_threads
    resources:
        mem_mb=256000, # Failed with 128GB
        runtime='4h' # Failed with 1h, most finish within 3h
    benchmark:
        benchmark_dir + "/metabuli/known{known_percent}/{sample}-"+str(num_threads)+"threads.benchmark"
    conda:
        "envs/metabuli.yml"
    log:
        output_dirs_dict['metabuli'] + "/logs/metabuli/known{known_percent}/{sample}.log"
    params:
        output_dir = output_dirs_dict['metabuli'],
    shell:
        "metabuli classify --threads {threads} {input.r1} {input.r2} {input.db} {params.output_dir}/output/known{wildcards.known_percent} {wildcards.sample} &> {log}"

rule metabuli_report_to_condensed:
    input:
        report=output_dirs_dict['metabuli'] + "/output/known{known_percent}/{sample}_report.tsv",
    output:
        profile = output_dirs_dict['metabuli'] + "/metabuli/known{known_percent}/{sample}.profile",
    resources:
        mem_mb=4000,
        runtime='1h'
    conda:
        "envs/singlem.yml"
    shell:
        "{workflow.basedir}/../bin/metabuli_to_condensed.py --input {input} " \
        "--bacterial-taxonomy {gtdb_bac_taxonomy} " \
        "--archaeal-taxonomy {gtdb_ar_taxonomy} > {output.profile}"

